{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "60d23c97",
            "metadata": {},
            "source": [
                "# Documentaci√≥n de Scrapers\n",
                "\n",
                "Este notebook contiene la documentaci√≥n y ejemplos de uso de los componentes de scraping y construcci√≥n de modelos de datos disponibles en la carpeta `src/scrapers`.\n",
                "\n",
                "## √çndice\n",
                "\n",
                "1. [Scraping de P√°ginas (Deep Scraper)](#scraping-de-paginas)\n",
                "   - Scraping de una sola p√°gina (`page_scraper`)\n",
                "   - Crawling recursivo (`page_deep_scraper`)\n",
                "2. [Construcci√≥n de Modelos (Model Builder)](#construccion-de-modelos)\n",
                "   - Identidad de la compa√±√≠a (`data_to_identity`)\n",
                "   - Generaci√≥n del modelo completo (`from_url_model`)\n",
                "3. [Utilidades de Extracci√≥n](#utilidades-de-extraccion)\n",
                "   - Overview de m√≥dulos de utilidad\n",
                "\n",
                "---\n",
                "\n",
                "*Nota: Este documento debe actualizarse a medida que se agreguen nuevas funcionalidades al m√≥dulo de scrapers.*"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "00485248",
            "metadata": {},
            "source": [
                "## ‚öôÔ∏è Configuraci√≥n Notebook\n",
                "Define el PATH para el correcto funcionamiento del notebook y la importaci√≥n de m√≥dulos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "44034166",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÅ Agregado al PYTHONPATH: C:\\Users\\Usuario\\Desktop\\data_scraper\\src\n"
                    ]
                }
            ],
            "source": [
                "# Configurar el PYTHONPATH para que Python encuentre los m√≥dulos\n",
                "import sys\n",
                "from pathlib import Path\n",
                "import os\n",
                "\n",
                "# Obtener la ruta del directorio src de forma robusta\n",
                "current_dir = Path(os.getcwd()).resolve()\n",
                "\n",
                "# Buscar el directorio ra√≠z que contiene 'scrapers'\n",
                "src_path = None\n",
                "search_dir = current_dir\n",
                "\n",
                "# Buscar hasta 3 niveles arriba\n",
                "for _ in range(3):\n",
                "    if (search_dir / 'scrapers').exists():\n",
                "        src_path = search_dir\n",
                "        break\n",
                "    search_dir = search_dir.parent\n",
                "\n",
                "# Si no se encontr√≥, usar el directorio actual si estamos dentro de scr\n",
                "if src_path is None:\n",
                "    if current_dir.name == 'src':\n",
                "        src_path = current_dir\n",
                "    else:\n",
                "        # Asumir que estamos en src/scrapers y subir un nivel\n",
                "        src_path = current_dir.parent if current_dir.name == 'scrapers' else current_dir\n",
                "\n",
                "# Agregar src al PYTHONPATH si no est√° ya\n",
                "if src_path and str(src_path) not in sys.path:\n",
                "    sys.path.insert(0, str(src_path))\n",
                "    print(f\"üìÅ Agregado al PYTHONPATH: {src_path}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "85e3a4df",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Scraping de P√°ginas (Deep Scraper) <a name=\"scraping-de-paginas\"></a>\n",
                "\n",
                "El m√≥dulo `page_deep_scraper.py` se encarga de la extracci√≥n de contenido web. Funciona en dos niveles:\n",
                "1. **Nivel P√°gina**: Extrae links y textos de una URL espec√≠fica, segmentando por `head`, `header`, `main` y `footer`.\n",
                "2. **Nivel Sitio (Crawler)**: Navega recursivamente por un dominio (BFS) para mapear su estructura."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "ef2ed7f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Importar funciones de scraping\n",
                "from scrapers.page_deep_scraper import page_scraper, page_deep_scraper\n",
                "import json\n",
                "\n",
                "# Funci√≥n auxiliar para imprimir JSON bonito\n",
                "def print_json(data):\n",
                "    print(json.dumps(data, indent=2, ensure_ascii=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c9197d34",
            "metadata": {},
            "source": [
                "### `page_scraper(url_base)`\n",
                "\n",
                "Extrae contenido de **una sola URL**.\n",
                "\n",
                "**Retorna:** Un objeto de p√°gina con:\n",
                "- `page`: La URL procesada.\n",
                "- `links`: Diccionario con listas de URLs encontradas en `head`, `header`, `main`, `footer`.\n",
                "- `texts`: Diccionario con listas de textos encontrados en las mismas secciones."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "116e9dee",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ P√°gina scrapeada: https://fraccional.cl\n",
                        "\n",
                        "Secciones con links:\n",
                        "  - head: 1 links\n",
                        "  - header: 9 links\n",
                        "  - main: 159 links\n",
                        "  - footer: 0 links\n",
                        "\n",
                        "Secciones con textos:\n",
                        "  - head: 0 textos\n",
                        "  - header: 17 textos\n",
                        "  - main: 544 textos\n",
                        "  - footer: 4 textos\n"
                    ]
                }
            ],
            "source": [
                "# Ejemplo: Scrapear una sola p√°gina\n",
                "TEST_URL = \"https://fraccional.cl\"\n",
                "\n",
                "try:\n",
                "    page_data = await page_scraper(TEST_URL)\n",
                "    \n",
                "    print(f\"‚úÖ P√°gina scrapeada: {page_data['page']}\")\n",
                "    print(\"\\nSecciones con links:\")\n",
                "    for section, links in page_data['links'].items():\n",
                "        print(f\"  - {section}: {len(links)} links\")\n",
                "        \n",
                "    print(\"\\nSecciones con textos:\")\n",
                "    for section, texts in page_data['texts'].items():\n",
                "        print(f\"  - {section}: {len(texts)} textos\")\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "36e2424f",
            "metadata": {},
            "source": [
                "### `page_deep_scraper(url_base, max_pages=100)`\n",
                "\n",
                "Realiza un **crawl recursivo** (Breadth-First Search) comenzando desde `url_base`.\n",
                "\n",
                "**Caracter√≠sticas:**\n",
                "- Respeta el **dominio ra√≠z** (no sale del sitio).\n",
                "- Detecta y evita ciclos (visita cada URL una sola vez).\n",
                "- Limita la cantidad de p√°ginas visitadas con `max_pages`.\n",
                "- Captura errores por p√°gina sin detener el proceso global.\n",
                "\n",
                "**Retorna:** Un objeto resumen con:\n",
                "- `rootDomain`: El dominio base.\n",
                "- `pagesScraped`: Cantidad total de p√°ginas procesadas.\n",
                "- `allInternalLinks`: Todos los links internos √∫nicos encontrados.\n",
                "- `pages`: Diccionario con el detalle de cada p√°gina (resultado de `page_scraper`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "bee39eba",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üåé Root Domain: fraccional.cl\n",
                        "üìÑ P√°ginas scrapeadas: 50\n",
                        "üîó Total links internos √∫nicos: 416\n",
                        "\n",
                        "URLs visitadas:\n",
                        "  - https://fraccional.cl\n",
                        "  - https://fraccional.cl/proyectos?address_level3=%C3%91u%C3%B1oa\n",
                        "  - https://fraccional.cl/gestores/zuba-inversiones\n",
                        "  - https://fraccional.cl/proyectos?address_level3=Asunci%C3%B3n\n",
                        "  - https://fraccional.cl/blog/posts/como-gana-plata-fraccional-2\n",
                        "  - https://fraccional.cl/blog/posts/el-secreto-detras-de-su-ascenso-al-top-1-de-la-riqueza-mundial\n",
                        "  - https://forms.fraccional.cl/geniobot\n",
                        "  - https://fraccional.cl/blog/posts/la-forma-sigue-las-finanzas\n",
                        "  - https://fraccional.cl/blog/posts/como-invertir-en-btc\n",
                        "  - https://fraccional.cl/blog/posts/que-debes-declarar-si-compraste-en-fraccional\n",
                        "  - https://fraccional.cl/blog/posts/alternativas-de-inversiones\n",
                        "  - https://fraccional.cl/pt-BR\n",
                        "  - https://fraccional.cl/proyectos/unidades/paraguay-zuba-13/235\n",
                        "  - https://fraccional.cl/blog/posts/como-china-destruyo-su-mercado-inmobiliario-en-tiempo-record\n",
                        "  - https://fraccional.cl/blog/posts/cuanto-aumentan-los-costos-sin-garantias\n",
                        "  - https://fraccional.cl/blog/posts/cuenta-de-ahorro-casa\n",
                        "  - https://fraccional.cl/blog/posts/volatilidad-inversiones\n",
                        "  - https://fraccional.cl/blog/posts/tasacion-de-propiedades\n",
                        "  - https://fraccional.cl/blog/posts/la-verdad-detras-de-los-mega-edificios-de-estacion-central\n",
                        "  - https://fraccional.cl/gestores/urvana\n",
                        "  - https://fraccional.cl/proyectos?categorias=short_term_rental\n",
                        "  - https://fraccional.cl/gestores/furo\n",
                        "  - https://fraccional.cl/blog/posts/aplicaciones-para-inversiones\n",
                        "  - https://fraccional.cl/web-app\n",
                        "  - https://fraccional.cl/blog/posts/como-calculamos-la-rentabilidad\n",
                        "  - https://fraccional.cl/blog/posts/canon-de-arriendo\n",
                        "  - https://fraccional.cl/blog/posts/fondos-indexados\n",
                        "  - https://fraccional.cl/blog/posts/como-elegir-propiedades-de-inversion\n",
                        "  - https://fraccional.cl/blog/posts/fraccional-cambio-de-look\n",
                        "  - https://fraccional.cl/blog/posts/noticias-fraccional-3\n",
                        "  - https://fraccional.cl/cmf\n",
                        "  - https://fraccional.cl/blog/posts/que-es-fraccional\n",
                        "  - https://fraccional.cl/blog/posts/tokenizacion-de-inmuebles\n",
                        "  - https://fraccional.cl/proyectos\n",
                        "  - https://fraccional.cl/blog/posts/blog-de-fraccional\n",
                        "  - https://fraccional.cl/proyectos?categorias=land_plot\n",
                        "  - https://fraccional.cl/blog/posts/estacionamientos-negocio\n",
                        "  - https://fraccional.cl/proyectos/unidades/losleones-606/59\n",
                        "  - https://fraccional.cl/blog/posts/el-circulo-vicioso-detras-del-precio-de-tu-casa\n",
                        "  - https://fraccional.cl/blog/posts/el-arriendo-es-gratuito-en-corea-del-sur\n",
                        "  - https://fraccional.cl/blog/posts/pie-en-cuotas\n",
                        "  - https://fraccional.cl/blog/posts/como-invertir-en-renta-corta\n",
                        "  - https://fraccional.cl/blog/posts/la-carrera-inmobiliaria-de-la-rata\n",
                        "  - https://fraccional.cl/blog/posts/millennials-jubilaran-arrendando\n",
                        "  - https://fraccional.cl/blog/posts/remate-inmobiliario\n",
                        "  - https://fraccional.cl/gestores/copahue\n",
                        "  - https://fraccional.cl/gestores/fixme\n",
                        "  - https://fraccional.cl/blog/posts/multicreditos-hipotecarios\n",
                        "  - https://fraccional.cl/blog/posts/la-vivienda-se-volvio-fea\n",
                        "  - https://fraccional.cl/proyectos?categorias=pe_fund\n"
                    ]
                }
            ],
            "source": [
                "# Ejemplo: Deep Scraper (limitado a 3 p√°ginas para demo)\n",
                "try:\n",
                "    deep_data = await page_deep_scraper(TEST_URL, max_pages=50)\n",
                "    \n",
                "    print(f\"üåé Root Domain: {deep_data['rootDomain']}\")\n",
                "    print(f\"üìÑ P√°ginas scrapeadas: {deep_data['pagesScraped']}\")\n",
                "    print(f\"üîó Total links internos √∫nicos: {len(deep_data['allInternalLinks'])}\")\n",
                "    print(\"\\nURLs visitadas:\")\n",
                "    for url in deep_data['pages'].keys():\n",
                "        print(f\"  - {url}\")\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a71b9ce5",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Construcci√≥n de Modelos (Model Builder) <a name=\"construccion-de-modelos\"></a>\n",
                "\n",
                "El m√≥dulo `model_builder.py` transforma la data \"cruda\" del scraper en un modelo estructurado (Company Model) listo para ser almacenado en la base de datos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "e0c0f041",
            "metadata": {},
            "outputs": [],
            "source": [
                "from scrapers.model_builder import data_to_identity, from_url_model, page_deep_scraped_to_dataSources"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a34c98a4",
            "metadata": {},
            "source": [
                "### `data_to_identity(url, name, slug, primary_domain)`\n",
                "\n",
                "Normaliza y genera la identidad b√°sica de una compa√±√≠a (slug, name, primary_domain).\n",
                "\n",
                "**Reglas:**\n",
                "- `primaryDomain`: Se normaliza usando `tldextract` y se limpia de `www.`.\n",
                "- `slug`: Se genera a partir del dominio si no se provee. Se reemplazan espacios por guiones.\n",
                "- `name`: Se genera a partir del slug si no se provee (Capitalizado)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "648ab11a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üÜî Identidad Generada:\n",
                        "{\n",
                        "  \"slug\": \"negocio\",\n",
                        "  \"name\": \"Negocio\",\n",
                        "  \"primaryDomain\": \"negocio.com\"\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "identity = data_to_identity(\n",
                "    url=\"https://www.ejemplo.negocio.com/inicio\",\n",
                "    name=None, # Auto-generar\n",
                "    slug=None  # Auto-generar\n",
                ")\n",
                "\n",
                "print(\"üÜî Identidad Generada:\")\n",
                "print_json(identity)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "10ffd85e",
            "metadata": {},
            "source": [
                "### `from_url_model(url, name, slug, primary_domain)`\n",
                "\n",
                "Es la funci√≥n de alto nivel que orquesta todo el proceso:\n",
                "1. Genera la identidad.\n",
                "2. Ejecuta `page_deep_scraper`.\n",
                "3. Convierte el resultado en una lista de `dataSources` estandarizada.\n",
                "4. Retorna el objeto completo de la compa√±√≠a.\n",
                "\n",
                "**Estructura del Output:**\n",
                "```json\n",
                "{\n",
                "  \"slug\": \"...\",\n",
                "  \"name\": \"...\",\n",
                "  \"primaryDomain\": \"...\",\n",
                "  \"dataSources\": [\n",
                "     { \"url\": \"...\", \"links\": {...}, \"texts\": {...} },\n",
                "     ...\n",
                "  ]\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "06232e33",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Iniciando construcci√≥n del modelo (esto puede tardar unos segundos)...\n",
                        "‚úÖ Modelo construido exitosamente:\n",
                        "  Nombre: Fraccional\n",
                        "  Slug: fraccional\n",
                        "  DataSources: 587\n",
                        "\n",
                        "Ejemplo de DataSource[0]:\n",
                        "  URL: https://fraccional.cl\n",
                        "  Links Head: 1\n"
                    ]
                }
            ],
            "source": [
                "# Ejemplo: Crear modelo completo desde una URL (con l√≠mite para demo)\n",
                "# Nota: from_url_model llama a page_deep_scraper internamente.\n",
                "# Para este ejemplo, simularemos una llamada r√°pida.\n",
                "\n",
                "print(\"üöÄ Iniciando construcci√≥n del modelo (esto puede tardar unos segundos)...\")\n",
                "\n",
                "try:\n",
                "    company_model = await from_url_model(url=TEST_URL)\n",
                "    \n",
                "    print(\"‚úÖ Modelo construido exitosamente:\")\n",
                "    print(f\"  Nombre: {company_model['name']}\")\n",
                "    print(f\"  Slug: {company_model['slug']}\")\n",
                "    print(f\"  DataSources: {len(company_model['dataSources'])}\")\n",
                "    \n",
                "    # Ver un dataSource de ejemplo\n",
                "    if company_model['dataSources']:\n",
                "        print(\"\\nEjemplo de DataSource[0]:\")\n",
                "        first_ds = company_model['dataSources'][0]\n",
                "        print(f\"  URL: {first_ds['url']}\")\n",
                "        print(f\"  Links Head: {len(first_ds.get('links', {}).get('head', []))}\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3bc1a57f",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Utilidades de Extracci√≥n <a name=\"utilidades-de-extraccion\"></a>\n",
                "\n",
                "La carpeta `src/scrapers/utils` contiene herramientas de bajo nivel usadas por los scrapers. Estas pueden ser usadas independientemente para tareas espec√≠ficas o debugging.\n",
                "\n",
                "### Componentes Principales:\n",
                "\n",
                "- **`requestHTTP.py`**: Manejo de peticiones HTTP con `aiohttp` y `playwright` (en el futuro).\n",
                "- **`html_spliter...py`**: Divide el HTML crudo en secciones sem√°nticas (`head`, `header`, `main`, `footer`).\n",
                "- **`urls_extractor...py`**: Extrae todas las URLs crudas de un string HTML.\n",
                "- **`urls_utilities_cleaner.py`**: Limpia URLs (quita espacios, caracteres inv√°lidos).\n",
                "- **`text_extractor...py`**: Extrae texto visible limpio del HTML.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "9d6f95cd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "HTML descargado: 1712074 caracteres\n",
                        "Tama√±o de secciones:\n",
                        "  head: 11592 caracteres\n",
                        "  header: 40032 caracteres\n",
                        "  main: 1352639 caracteres\n",
                        "  footer: 1522 caracteres\n"
                    ]
                }
            ],
            "source": [
                "# Ejemplo: Uso directo de utilidades para debug\n",
                "from scrapers.utils.requestHTTP import fetch_html\n",
                "from scrapers.utils.html_spliter_head_header_main_footer import html_spliter_head_header_main_footer\n",
                "\n",
                "try:\n",
                "    # 1. Fetch HTML manual\n",
                "    raw_html = await fetch_html(TEST_URL)\n",
                "    print(f\"HTML descargado: {len(raw_html)} caracteres\")\n",
                "    \n",
                "    # 2. Split HTML\n",
                "    sections = html_spliter_head_header_main_footer(raw_html)\n",
                "    print(\"Tama√±o de secciones:\")\n",
                "    for sec, content in sections.items():\n",
                "        print(f\"  {sec}: {len(content)} caracteres\")\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fd56a3ae",
            "metadata": {},
            "source": [
                "---\n",
                "## Extensibilidad\n",
                "\n",
                "Para agregar nuevos scrapers o funcionalidades:\n",
                "\n",
                "1. **Nuevos Parsers**: Agregarlos en `src/scrapers/utils/` si son utilidades gen√©ricas de limpieza o extracci√≥n.\n",
                "2. **L√≥gica de Scraping**: Modificar `src/scrapers/page_deep_scraper.py` para mejorar la navegaci√≥n o la extracci√≥n de datos espec√≠fica.\n",
                "3. **Modelado**: Actualizar `src/scrapers/model_builder.py` si cambia la estructura de la base de datos (DataSources).\n",
                "\n",
                "Consulte `src/scrapers/estructura_recomendada.txt` para ver la estructura modular planificada para el futuro."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "envData",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
