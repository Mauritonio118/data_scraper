{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Documentaci√≥n de Scrapers\n",
    "\n",
    "Este notebook contiene la documentaci√≥n y ejemplos de uso de los componentes de scraping y construcci√≥n de modelos de datos disponibles en la carpeta `src/scrapers`.\n",
    "\n",
    "## √çndice\n",
    "\n",
    "1. [Scraping de P√°ginas (Deep Scraper)](#scraping-de-paginas)\n",
    "   - Scraping de una sola p√°gina (`page_scraper`)\n",
    "   - Crawling recursivo (`page_deep_scraper`)\n",
    "2. [Construcci√≥n de Modelos (Model Builder)](#construccion-de-modelos)\n",
    "   - Identidad de la compa√±√≠a (`data_to_identity`)\n",
    "   - Generaci√≥n del modelo completo (`from_url_model`)\n",
    "3. [Utilidades de Extracci√≥n](#utilidades-de-extraccion)\n",
    "   - Overview de m√≥dulos de utilidad\n",
    "\n",
    "4. [Scraping de Perfiles Externos](#scraping-de-perfiles-externos)\n",
    "   - Extracci√≥n de TheCrowdSpace (`thecrowdspace_profile_scraper`)\n",
    "5. [Scrapers Espec√≠ficos](#scrapers-especificos)\n",
    "   - Extracci√≥n de Favicon (`get_favicon_url`)\n",
    "---\n",
    "\n",
    "*Nota: Este documento debe actualizarse a medida que se agreguen nuevas funcionalidades al m√≥dulo de scrapers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuraci√≥n Notebook\n",
    "Define el PATH para el correcto funcionamiento del notebook y la importaci√≥n de m√≥dulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se asume que el paquete 'src' esta instalado en el entorno (pip install -e .)\n",
    "import sys\n",
    "print(f\"‚úÖ Python executable: {sys.executable}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Scraping de P√°ginas (Deep Scraper) <a name=\"scraping-de-paginas\"></a>\n",
    "\n",
    "El m√≥dulo `page_deep_scraper.py` se encarga de la extracci√≥n de contenido web. Funciona en dos niveles:\n",
    "1. **Nivel P√°gina**: Extrae links y textos de una URL espec√≠fica, segmentando por `head`, `header`, `main` y `footer`.\n",
    "2. **Nivel Sitio (Crawler)**: Navega recursivamente por un dominio (BFS) para mapear su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar funciones de scraping\n",
    "from src.scrapers.page_deep_scraper import page_scraper, page_deep_scraper\n",
    "import json\n",
    "\n",
    "# Funci√≥n auxiliar para imprimir JSON bonito\n",
    "def print_json(data):\n",
    "    print(json.dumps(data, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### `page_scraper(url_base)`\n",
    "\n",
    "Extrae contenido de **una sola URL**.\n",
    "\n",
    "**Retorna:** Un objeto de p√°gina con:\n",
    "- `page`: La URL procesada.\n",
    "- `links`: Diccionario con listas de URLs encontradas en `head`, `header`, `main`, `footer`.\n",
    "- `texts`: Diccionario con listas de textos encontrados en las mismas secciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Scrapear una sola p√°gina\n",
    "TEST_URL = \"https://fraccional.cl\"\n",
    "\n",
    "try:\n",
    "    page_data = await page_scraper(TEST_URL)\n",
    "    \n",
    "    print(f\"‚úÖ P√°gina scrapeada: {page_data['page']}\")\n",
    "    print(\"\\nSecciones con links:\")\n",
    "    for section, links in page_data['links'].items():\n",
    "        print(f\"  - {section}: {len(links)} links\")\n",
    "        \n",
    "    print(\"\\nSecciones con textos:\")\n",
    "    for section, texts in page_data['texts'].items():\n",
    "        print(f\"  - {section}: {len(texts)} textos\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### `page_deep_scraper(url_base, max_pages=100)`\n",
    "\n",
    "Realiza un **crawl recursivo** (Breadth-First Search) comenzando desde `url_base`.\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- Respeta el **dominio ra√≠z** (no sale del sitio).\n",
    "- Detecta y evita ciclos (visita cada URL una sola vez).\n",
    "- Limita la cantidad de p√°ginas visitadas con `max_pages`.\n",
    "- Captura errores por p√°gina sin detener el proceso global.\n",
    "\n",
    "**Retorna:** Un objeto resumen con:\n",
    "- `rootDomain`: El dominio base.\n",
    "- `pagesScraped`: Cantidad total de p√°ginas procesadas.\n",
    "- `allInternalLinks`: Todos los links internos √∫nicos encontrados.\n",
    "- `pages`: Diccionario con el detalle de cada p√°gina (resultado de `page_scraper`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Deep Scraper (limitado a 3 p√°ginas para demo)\n",
    "try:\n",
    "    deep_data = await page_deep_scraper(TEST_URL, max_pages=50)\n",
    "    \n",
    "    print(f\"üåé Root Domain: {deep_data['rootDomain']}\")\n",
    "    print(f\"üìÑ P√°ginas scrapeadas: {deep_data['pagesScraped']}\")\n",
    "    print(f\"üîó Total links internos √∫nicos: {len(deep_data['allInternalLinks'])}\")\n",
    "    print(\"\\nURLs visitadas:\")\n",
    "    for url in deep_data['pages'].keys():\n",
    "        print(f\"  - {url}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Construcci√≥n de Modelos (Model Builder) <a name=\"construccion-de-modelos\"></a>\n",
    "\n",
    "El m√≥dulo `model_builder.py` transforma la data \"cruda\" del scraper en un modelo estructurado (Company Model) listo para ser almacenado en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scrapers.model_builder import data_to_identity, from_url_model, page_deep_scraped_to_dataSources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### `data_to_identity(url, name, slug, primary_domain)`\n",
    "\n",
    "Normaliza y genera la identidad b√°sica de una compa√±√≠a (slug, name, primary_domain).\n",
    "\n",
    "**Reglas:**\n",
    "- `primaryDomain`: Se normaliza usando `tldextract` y se limpia de `www.`.\n",
    "- `slug`: Se genera a partir del dominio si no se provee. Se reemplazan espacios por guiones.\n",
    "- `name`: Se genera a partir del slug si no se provee (Capitalizado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = data_to_identity(\n",
    "    url=\"https://www.ejemplo.negocio.com/inicio\",\n",
    "    name=None, # Auto-generar\n",
    "    slug=None  # Auto-generar\n",
    ")\n",
    "\n",
    "print(\"üÜî Identidad Generada:\")\n",
    "print_json(identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### `from_url_model(url, name, slug, primary_domain)`\n",
    "\n",
    "Es la funci√≥n de alto nivel que orquesta todo el proceso:\n",
    "1. Genera la identidad.\n",
    "2. Ejecuta `page_deep_scraper`.\n",
    "3. Convierte el resultado en una lista de `dataSources` estandarizada.\n",
    "4. Retorna el objeto completo de la compa√±√≠a.\n",
    "\n",
    "**Estructura del Output:**\n",
    "```json\n",
    "{\n",
    "  \"slug\": \"...\",\n",
    "  \"name\": \"...\",\n",
    "  \"primaryDomain\": \"...\",\n",
    "  \"dataSources\": [\n",
    "     { \"url\": \"...\", \"links\": {...}, \"texts\": {...} },\n",
    "     ...\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Crear modelo completo desde una URL (con l√≠mite para demo)\n",
    "# Nota: from_url_model llama a page_deep_scraper internamente.\n",
    "# Para este ejemplo, simularemos una llamada r√°pida.\n",
    "\n",
    "print(\"üöÄ Iniciando construcci√≥n del modelo (esto puede tardar unos segundos)...\")\n",
    "\n",
    "try:\n",
    "    company_model = await from_url_model(url=TEST_URL)\n",
    "    \n",
    "    print(\"‚úÖ Modelo construido exitosamente:\")\n",
    "    print(f\"  Nombre: {company_model['name']}\")\n",
    "    print(f\"  Slug: {company_model['slug']}\")\n",
    "    print(f\"  DataSources: {len(company_model['dataSources'])}\")\n",
    "    \n",
    "    # Ver un dataSource de ejemplo\n",
    "    if company_model['dataSources']:\n",
    "        print(\"\\nEjemplo de DataSource[0]:\")\n",
    "        first_ds = company_model['dataSources'][0]\n",
    "        print(f\"  URL: {first_ds['url']}\")\n",
    "        print(f\"  Links Head: {len(first_ds.get('links', {}).get('head', []))}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Utilidades de Extracci√≥n <a name=\"utilidades-de-extraccion\"></a>\n",
    "\n",
    "La carpeta `src/scrapers/utils` contiene herramientas de bajo nivel usadas por los scrapers. Estas pueden ser usadas independientemente para tareas espec√≠ficas o debugging.\n",
    "\n",
    "### Componentes Principales:\n",
    "\n",
    "- **`requestHTTP.py`**: Manejo de peticiones HTTP con `aiohttp` y `playwright` (en el futuro).\n",
    "- **`html_spliter...py`**: Divide el HTML crudo en secciones sem√°nticas (`head`, `header`, `main`, `footer`).\n",
    "- **`urls_extractor...py`**: Extrae todas las URLs crudas de un string HTML.\n",
    "- **`urls_utilities_cleaner.py`**: Limpia URLs (quita espacios, caracteres inv√°lidos).\n",
    "- **`text_extractor...py`**: Extrae texto visible limpio del HTML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Uso directo de utilidades para debug\n",
    "from src.scrapers.utils.requestHTTP import fetch_html\n",
    "from src.scrapers.utils.html_spliter_head_header_main_footer import html_spliter_head_header_main_footer\n",
    "\n",
    "try:\n",
    "    # 1. Fetch HTML manual\n",
    "    raw_html = await fetch_html(TEST_URL)\n",
    "    print(f\"HTML descargado: {len(raw_html)} caracteres\")\n",
    "    \n",
    "    # 2. Split HTML\n",
    "    sections = html_spliter_head_header_main_footer(raw_html)\n",
    "    print(\"Tama√±o de secciones:\")\n",
    "    for sec, content in sections.items():\n",
    "        print(f\"  {sec}: {len(content)} caracteres\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Scraping de Perfiles Externos <a name=\"scraping-de-perfiles-externos\"></a>\n",
    "\n",
    "Este m√≥dulo permite la extracci√≥n de informaci√≥n detallada desde perfiles de terceros, espec√≠ficamente de **TheCrowdSpace**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scrapers.thecrowdspace_profile_scraper import thecrowdspace_profile_scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### `thecrowdspace_profile_scraper(url, html_content=None)`\n",
    "\n",
    "Extrae estructuradamente data de un perfil p√∫blico de TheCrowdSpace.\n",
    "\n",
    "**Argumentos:**\n",
    "- `url`: URL del perfil.\n",
    "- `html_content`: (Opcional) HTML crudo si ya fue descargado previamente.\n",
    "\n",
    "**Retorna:** Un diccionario completo que coincide directamente con el campo `theCrowdSpace` del modelo de las compa√±√≠as, con secciones:\n",
    "- `hero`: Logo, verificaci√≥n, industrias.\n",
    "- `content`: Tarjetas superiores y descripciones largas.\n",
    "- `sidebar`: Estad√≠sticas, regi√≥n operativa, links sociales, estado, etc.\n",
    "- `p2p`: Scores y an√°lisis de riesgos.\n",
    "- `team`: Lista de miembros y cargos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Scrapear un perfil de TheCrowdSpace\n",
    "CROWDSPACE_URL = \"https://thecrowdspace.com/platform/bergfurst\"\n",
    "\n",
    "try:\n",
    "    print(f\"üïµÔ∏è‚Äç‚ôÄÔ∏è Scrapeando: {CROWDSPACE_URL} ...\")\n",
    "    # Nota: Si devuelve vac√≠o puede ser por bloqueo de bot o URL inv√°lida.\n",
    "    # Se recomienda usar headers reales o un proxy en producci√≥n.\n",
    "    profile_data = thecrowdspace_profile_scraper(CROWDSPACE_URL)\n",
    "    \n",
    "    if profile_data:\n",
    "        print(\"‚úÖ Data extra√≠da exitosamente!\")\n",
    "        \n",
    "        print(f\"\\nüè¢ Nombre/Web detectada para: {profile_data.get('theCrowdSpaceUrl')}\")\n",
    "        \n",
    "        if 'hero' in profile_data:\n",
    "             print(f\"  Industrias: {profile_data['hero'].get('industries')}\")\n",
    "             \n",
    "        if 'sidebar' in profile_data:\n",
    "             sb = profile_data['sidebar']\n",
    "             print(f\"  Website: {sb.get('websiteUrl')}\")\n",
    "             print(f\"  Status: {sb.get('status')}\")\n",
    "             if 'operatesIn' in sb:\n",
    "                 print(f\"  Operates In: {sb['operatesIn'].get('countries')}\")\n",
    "        \n",
    "        if 'team' in profile_data:\n",
    "             print(f\"  Team Members: {len(profile_data['team'])}\")\n",
    "             \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No se pudo extraer informaci√≥n o el perfil est√° vac√≠o.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Scrapers Espec√≠ficos <a name=\"scrapers-especificos\"></a>\n",
    "\n",
    "Scrapers dise√±ados para tareas puntuales de extracci√≥n de recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scrapers.favicon_scraper import get_favicon_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### `get_favicon_url(url)`\n",
    "\n",
    "Busca y valida el favicon de una URL dada. Implementa heur√≠sticas robustas:\n",
    "1. Busca tags `<link rel=\"icon\" ...>` en el HTML.\n",
    "2. Manejo de rutas relativas y absolutas.\n",
    "3. Fallback a `/favicon.ico` en la ra√≠z del dominio.\n",
    "4. Validaci√≥n final por status HTTP 200.\n",
    "\n",
    "**Retorna:** String con la URL absoluta del favicon o lanza Exception si falla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Obtener favicon\n",
    "TARGET_URL = \"https://google.com\"\n",
    "\n",
    "try:\n",
    "    favicon = get_favicon_url(TARGET_URL)\n",
    "    print(f\"‚úÖ Favicon encontrado: {favicon}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "---\n",
    "## Extensibilidad\n",
    "\n",
    "Para agregar nuevos scrapers o funcionalidades:\n",
    "\n",
    "1. **Nuevos Parsers**: Agregarlos en `src/scrapers/utils/` si son utilidades gen√©ricas de limpieza o extracci√≥n.\n",
    "2. **L√≥gica de Scraping**: Modificar `src/scrapers/page_deep_scraper.py` para mejorar la navegaci√≥n o la extracci√≥n de datos espec√≠fica.\n",
    "3. **Modelado**: Actualizar `src/scrapers/model_builder.py` si cambia la estructura de la base de datos (DataSources).\n",
    "\n",
    "Consulte `src/scrapers/estructura_recomendada.txt` para ver la estructura modular planificada para el futuro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
